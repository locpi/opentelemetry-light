# Configuration Tempo pour les Spans Java

## ‚úÖ Am√©liorations Apport√©es

Votre configuration Tempo a √©t√© optimis√©e pour mieux afficher les spans des applications Java.

### 1. **Configuration des Receivers OTLP**

```yaml
distributor:
  receivers:
    otlp:
      protocols:
        http:
          endpoint: 0.0.0.0:4318  # Explicitement d√©fini
        grpc:
          endpoint: 0.0.0.0:4317  # Explicitement d√©fini
```

‚úÖ Les endpoints OTLP sont maintenant explicites pour recevoir les traces Java.

### 2. **Optimisation de l'Ingester pour Java**

```yaml
ingester:
  max_block_duration: 5m
  trace_idle_period: 20s       # ‚≠ê Nouveau : Temps d'attente avant de compl√©ter une trace
  max_block_bytes: 1_000_000   # ‚≠ê Nouveau : Taille max des blocs
```

**Pourquoi ?** Les traces Java peuvent avoir beaucoup de spans (REST + BDD + m√©thodes). Ces param√®tres permettent de :
- Attendre plus longtemps que tous les spans arrivent avant de finaliser la trace
- G√©rer des traces plus volumineuses

### 3. **Metrics Generator Am√©lior√©**

```yaml
metrics_generator:
  processor:
    service_graphs:
      dimensions:
        - http.method          # GET, POST, etc.
        - http.status_code     # 200, 404, 500, etc.
        - service.namespace    # weeding-app, etc.

    span_metrics:
      dimensions:
        - http.method
        - http.status_code
        - service.name
        - service.namespace
        - db.system           # ‚≠ê PostgreSQL, MySQL, MongoDB
        - db.name             # ‚≠ê Nom de la BDD
      enable_target_info: true
```

**R√©sultat** : Vous pouvez maintenant :
- Visualiser le graphe des services (qui appelle qui)
- Voir les m√©triques par m√©thode HTTP et statut
- Filtrer par base de donn√©es utilis√©e

### 4. **Limites Augment√©es pour Java**

```yaml
overrides:
  defaults:
    ingestion:
      max_bytes_per_trace: 5_000_000      # ‚≠ê 5 MB (au lieu de ~1 MB par d√©faut)
      max_spans_per_trace: 100_000        # ‚≠ê 100k spans max

    metrics_generator:
      max_trace_duration: 5m              # ‚≠ê Traces jusqu'√† 5 minutes
```

**Pourquoi ?** Les applications Java instrument√©es peuvent g√©n√©rer beaucoup de spans :
- Un endpoint REST qui appelle 3 services et fait 5 requ√™tes BDD = ~50 spans minimum
- Avec l'auto-instrumentation, chaque m√©thode peut cr√©er un span

### 5. **Configuration Query pour Grafana**

```yaml
query_frontend:
  search:
    max_duration: 0s              # Pas de limite de dur√©e de recherche
    default_result_limit: 20      # 20 traces par d√©faut
  trace_by_id:
    query_timeout: 30s            # Timeout de 30s pour chercher une trace

querier:
  max_concurrent_queries: 10      # 10 requ√™tes simultan√©es max
```

**R√©sultat** : Recherche fluide dans Grafana m√™me avec beaucoup de traces.

## üéØ Configuration Grafana Am√©lior√©e

### Tags Kubernetes Automatiques

```yaml
tracesToLogs:
  tags: ['service.name', 'k8s.namespace.name', 'k8s.pod.name', 'k8s.deployment.name']
  mappedTags:
    - key: 'service.name'
      value: 'service'
    - key: 'k8s.namespace.name'
      value: 'namespace'
  mapTagNamesEnabled: true
```

**R√©sultat** : Depuis une trace, vous pouvez cliquer pour voir les logs du pod exact.

### M√©triques depuis les Spans

```yaml
tracesToMetrics:
  queries:
    - name: 'Request Rate'
      query: 'rate(tempo_spanmetrics_calls_total{$__tags}[5m])'
    - name: 'Request Duration p95'
      query: 'histogram_quantile(0.95, rate(tempo_spanmetrics_latency_bucket{$__tags}[5m]))'
    - name: 'Error Rate'
      query: 'rate(tempo_spanmetrics_calls_total{$__tags, status_code="STATUS_CODE_ERROR"}[5m])'
```

**R√©sultat** : Depuis une trace dans Grafana, vous voyez directement les m√©triques RED (Rate, Errors, Duration).

### Corr√©lation Logs ‚Üî Traces

```yaml
# Dans Loki datasource
derivedFields:
  - datasourceUid: tempo
    matcherRegex: "trace_id=(\\w+)"
    name: TraceID
    url: "$${__value.raw}"
```

**R√©sultat** : Dans les logs, un lien cliquable vers la trace correspondante.

## üìä Ce Que Vous Verrez Maintenant dans Grafana

### 1. Trace Compl√®te d'un Endpoint Java

```
backend-service: GET /api/users/123 (245ms)
  ‚îÇ
  ‚îú‚îÄ Spring MVC: GET /api/users/{id} (240ms)
  ‚îÇ   ‚îÇ
  ‚îÇ   ‚îú‚îÄ HTTP Client: GET auth-service/validate (45ms)
  ‚îÇ   ‚îÇ   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îî‚îÄ auth-service: POST /validate (40ms)
  ‚îÇ   ‚îÇ       ‚îÇ
  ‚îÇ   ‚îÇ       ‚îî‚îÄ JDBC: SELECT * FROM tokens WHERE... (8ms)
  ‚îÇ   ‚îÇ
  ‚îÇ   ‚îú‚îÄ UserService.getUser() (180ms)
  ‚îÇ   ‚îÇ   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îú‚îÄ JPA: UserRepository.findById() (35ms)
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ JDBC: SELECT * FROM users WHERE id=? (30ms)
  ‚îÇ   ‚îÇ   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îî‚îÄ HTTP Client: GET profile-service/profiles/123 (140ms)
  ‚îÇ   ‚îÇ       ‚îÇ
  ‚îÇ   ‚îÇ       ‚îî‚îÄ profile-service: GET /profiles/{id} (135ms)
  ‚îÇ   ‚îÇ           ‚îÇ
  ‚îÇ   ‚îÇ           ‚îî‚îÄ JDBC: SELECT * FROM profiles WHERE... (32ms)
  ‚îÇ   ‚îÇ
  ‚îÇ   ‚îî‚îÄ JSON Serialization (5ms)
  ‚îÇ
  ‚îî‚îÄ Response: 200 OK
```

### 2. Tags Disponibles pour Recherche

Vous pouvez rechercher les traces par :
- **service.name** : `backend-service`, `auth-service`
- **http.method** : `GET`, `POST`, `PUT`, `DELETE`
- **http.status_code** : `200`, `404`, `500`
- **http.url** : `/api/users/123`
- **db.system** : `postgresql`, `mysql`, `mongodb`
- **db.name** : Nom de votre base
- **k8s.namespace.name** : `weeding-app`
- **k8s.pod.name** : `backend-service-abc123`
- **error** : `true` (pour les spans en erreur)

### 3. Service Graph

Dans Grafana, vous pouvez visualiser :
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ frontend-app ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ backend-service  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ auth-service ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ profile-service  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

Avec m√©triques sur chaque lien :
- Nombre de requ√™tes/sec
- Latence moyenne
- Taux d'erreur

## üîç Comment Utiliser dans Grafana

### Rechercher des Traces

1. **Explore** ‚Üí Datasource **Tempo**
2. **Query type** : Search
3. Filtrer par :
   - **Service Name** : `backend-service`
   - **Span Name** : `GET /api/users/{id}`
   - **Duration** : `> 200ms` (traces lentes)
   - **Status** : `error` (traces avec erreurs)

### Voir le Service Graph

1. **Explore** ‚Üí Datasource **Tempo**
2. **Query type** : Service Graph
3. S√©lectionner la p√©riode
4. Voir le graphe des relations entre services

### Corr√©ler Trace ‚Üí Logs

1. Ouvrir une trace dans Tempo
2. Cliquer sur un span
3. Dans le panneau de droite : **Logs for this span**
4. Voir les logs du pod exact au moment du span

### Corr√©ler Trace ‚Üí M√©triques

1. Ouvrir une trace dans Tempo
2. Cliquer sur **Related Metrics**
3. Voir les requ√™tes PromQL avec les valeurs du span

## üöÄ D√©ployer les Changements

```bash
# Appliquer la nouvelle configuration
kubectl apply -f k8s/observability/03-tempo.yaml
kubectl apply -f k8s/observability/04-grafana.yaml

# Red√©marrer Tempo pour prendre en compte la config
kubectl rollout restart deployment/tempo -n opentelemetry

# Red√©marrer Grafana pour les datasources
kubectl rollout restart deployment/grafana -n opentelemetry

# Attendre que les pods soient pr√™ts
kubectl wait --for=condition=ready pod -l app=tempo -n opentelemetry --timeout=300s
kubectl wait --for=condition=ready pod -l app=grafana -n opentelemetry --timeout=300s
```

## ‚úÖ V√©rification

### 1. V√©rifier que Tempo re√ßoit les traces

```bash
# Voir les logs de Tempo
kubectl logs -n opentelemetry deployment/tempo -f

# Vous devriez voir :
# level=info msg="received OTLP trace request"
# level=info msg="ingester flushing trace"
```

### 2. Tester l'envoi de traces

```bash
# Depuis un pod ou votre machine (avec port-forward)
curl -X POST http://tempo.opentelemetry:3200/api/echo

# Devrait retourner : "echo"
```

### 3. V√©rifier les m√©triques g√©n√©r√©es

Dans Prometheus (http://localhost:9090), rechercher :

```promql
# M√©triques de spans
tempo_spanmetrics_calls_total

# Latence par service
tempo_spanmetrics_latency_bucket{service_name="backend-service"}

# Service graph
traces_service_graph_request_total
```

### 4. V√©rifier dans Grafana

1. Ouvrir Grafana : http://localhost:3000
2. **Explore** ‚Üí **Tempo**
3. Faire une recherche par service name
4. Vous devriez voir vos traces avec tous les spans !

## üêõ Troubleshooting

### Pas de traces visibles

1. **V√©rifier que l'application envoie des traces** :
```bash
kubectl logs -n weeding-app deployment/backend-service | grep -i "otel\|trace"
```

2. **V√©rifier que le collector re√ßoit les traces** :
```bash
kubectl logs -n opentelemetry deployment/otel-collector -f | grep -i trace
```

3. **V√©rifier que Tempo re√ßoit du collector** :
```bash
kubectl logs -n opentelemetry deployment/tempo -f | grep -i "received\|ingesting"
```

### Traces incompl√®tes (spans manquants)

‚û°Ô∏è **Cause** : `trace_idle_period` trop court ou traces trop longues.

‚û°Ô∏è **Solution** : Augmenter dans tempo.yaml :
```yaml
ingester:
  trace_idle_period: 30s      # Attendre 30s au lieu de 20s

overrides:
  defaults:
    metrics_generator:
      max_trace_duration: 10m  # Traces jusqu'√† 10min
```

### "Max trace size exceeded"

‚û°Ô∏è **Solution** : Augmenter les limites :
```yaml
overrides:
  defaults:
    ingestion:
      max_bytes_per_trace: 10_000_000    # 10 MB
      max_spans_per_trace: 200_000       # 200k spans
```

## üìö Ressources

- [Tempo Configuration](https://grafana.com/docs/tempo/latest/configuration/)
- [Tempo Metrics Generator](https://grafana.com/docs/tempo/latest/metrics-generator/)
- [Grafana Tempo Datasource](https://grafana.com/docs/grafana/latest/datasources/tempo/)

# Configuration Tempo Corrig√©e - Compatible avec Version 2.3.1

## ‚úÖ Probl√®mes R√©solus

### Erreurs de Configuration YAML

**Erreurs rencontr√©es** :
```
line 69: field max_trace_duration not found in type overrides.Limits
line 73: field max_spans_per_trace not found in type overrides.Limits
line 72: field query_timeout not found in type frontend.TraceByIDConfig
line 82: field defaults not found in type overrides.Limits
```

**Causes** :
- Version Tempo 2.2.3 ne supporte pas certains champs dans `overrides`
- Syntaxe incorrecte pour les overrides (pas de structure `defaults`)
- Champs non support√©s dans `query_frontend`

## üìù Configuration Finale Valide

### Structure YAML Correcte

```yaml
server:
  http_listen_port: 3200
  log_level: info

distributor:
  receivers:
    otlp:
      protocols:
        http:
          endpoint: 0.0.0.0:4318
        grpc:
          endpoint: 0.0.0.0:4317

ingester:
  max_block_duration: 5m
  trace_idle_period: 20s        # ‚úÖ Important pour traces Java multi-spans
  max_block_bytes: 1_000_000    # ‚úÖ G√©rer traces volumineuses

compactor:
  compaction:
    block_retention: 1h

metrics_generator:
  registry:
    external_labels:
      source: tempo
      cluster: minikube
  storage:
    path: /var/tempo/generator/wal
    remote_write:
      - url: http://prometheus:9090/api/v1/write
        send_exemplars: true
  processor:
    service_graphs:
      dimensions:
        - http.method
        - http.status_code
    span_metrics:
      dimensions:
        - http.method
        - http.status_code
        - service.name

storage:
  trace:
    backend: local
    wal:
      path: /var/tempo/wal
    local:
      path: /var/tempo/blocks
    pool:
      max_workers: 100          # ‚úÖ Optimisation pour Java
      queue_depth: 10000

query_frontend:
  search:
    max_duration: 0s
    default_result_limit: 20

querier:
  max_concurrent_queries: 10

overrides:
  metrics_generator_processors: [service-graphs, span-metrics]
```

## üîÑ Changements Appliqu√©s

### 1. Version Tempo Mise √† Jour

**Avant** : `grafana/tempo:2.2.3`
**Apr√®s** : `grafana/tempo:2.3.1`

Version plus r√©cente avec meilleur support des fonctionnalit√©s.

### 2. Configuration `overrides` Simplifi√©e

**Supprim√©** :
- `max_trace_duration` (non support√© dans overrides pour cette version)
- `max_bytes_per_trace` (non support√© dans overrides pour cette version)
- `max_spans_per_trace` (non support√© dans overrides pour cette version)
- Structure `defaults` (syntaxe incorrecte)

**Conserv√©** :
- `metrics_generator_processors` : Active les m√©triques de service graphs et spans

### 3. Configuration `ingester` Optimis√©e

Ces param√®tres **fonctionnent** et sont importants pour Java :

```yaml
ingester:
  max_block_duration: 5m
  trace_idle_period: 20s      # Attend 20s pour collecter tous les spans
  max_block_bytes: 1_000_000  # 1 MB par bloc (suffisant pour la plupart des traces Java)
```

**Pourquoi c'est important** :
- Les traces Java avec auto-instrumentation g√©n√®rent beaucoup de spans
- Un endpoint REST ‚Üí appel service ‚Üí BDD peut cr√©er 20-50 spans
- `trace_idle_period: 20s` laisse le temps √† tous les spans d'arriver avant de finaliser la trace

### 4. `metrics_generator` Fonctionnel

```yaml
metrics_generator:
  processor:
    service_graphs:
      dimensions:
        - http.method
        - http.status_code
    span_metrics:
      dimensions:
        - http.method
        - http.status_code
        - service.name
```

**R√©sultat** : Vous obtiendrez des m√©triques Prometheus :
- `tempo_spanmetrics_calls_total` : Nombre d'appels par service
- `tempo_spanmetrics_latency_bucket` : Histogramme de latence
- `traces_service_graph_request_total` : Relations entre services

## üöÄ D√©ploiement

```bash
# 1. Appliquer la configuration corrig√©e
kubectl apply -f k8s/observability/03-tempo.yaml

# 2. Attendre que le nouveau pod d√©marre
kubectl rollout status deployment/tempo -n opentelemetry

# 3. V√©rifier qu'il n'y a plus d'erreurs
kubectl logs -n opentelemetry deployment/tempo --tail=50

# Vous devriez voir :
# level=info msg="Tempo started"
# level=info msg="Server listening on addresses"
```

## ‚úÖ V√©rification

### 1. V√©rifier que Tempo d√©marre sans erreur

```bash
kubectl logs -n opentelemetry deployment/tempo

# Devrait afficher :
# level=info msg="Tempo started"
# level=info msg="OTLP receiver started"
```

### 2. Tester la r√©ception OTLP

```bash
# Port-forward
kubectl port-forward -n opentelemetry svc/tempo 3200:3200

# Tester l'API
curl http://localhost:3200/api/echo
# Devrait retourner : "echo"
```

### 3. V√©rifier dans Grafana

1. Ouvrir Grafana : http://localhost:3000
2. **Explore** ‚Üí datasource **Tempo**
3. Faire un test de recherche

Si vous voyez des traces, c'est bon ! ‚úÖ

## üìä Ce Que Vous Obtiendrez

### Traces Java Compl√®tes

Avec cette configuration, vos traces contiendront :

```
backend-service: GET /api/users/123 (245ms)
  ‚îú‚îÄ Spring MVC: GET /api/users/{id} (240ms)
  ‚îÇ   ‚îú‚îÄ HTTP Client: GET auth-service/validate (45ms)
  ‚îÇ   ‚îÇ   ‚îî‚îÄ auth-service: POST /validate (40ms)
  ‚îÇ   ‚îÇ       ‚îî‚îÄ JDBC Query (8ms)
  ‚îÇ   ‚îî‚îÄ UserService.getUser() (180ms)
  ‚îÇ       ‚îú‚îÄ JPA: findById() (35ms)
  ‚îÇ       ‚îÇ   ‚îî‚îÄ JDBC: SELECT * FROM users (30ms)
  ‚îÇ       ‚îî‚îÄ HTTP Client: GET profile-service/... (140ms)
  ‚îî‚îÄ Response 200
```

### M√©triques G√©n√©r√©es

Dans Prometheus, vous aurez :

```promql
# Nombre d'appels par service
tempo_spanmetrics_calls_total{service_name="backend-service"}

# Latence p95
histogram_quantile(0.95,
  rate(tempo_spanmetrics_latency_bucket{service_name="backend-service"}[5m])
)

# Graphe des services
traces_service_graph_request_total{client="backend-service", server="auth-service"}
```

### Service Graph dans Grafana

Visualisation des relations entre services :
```
frontend ‚Üí backend-service ‚Üí auth-service
              ‚Üì
         profile-service
```

## üêõ Troubleshooting

### Tempo ne d√©marre toujours pas

```bash
# Voir les logs d√©taill√©s
kubectl logs -n opentelemetry deployment/tempo -f

# V√©rifier la config
kubectl get configmap tempo-config -n opentelemetry -o yaml
```

### Erreur "field X not found"

‚û°Ô∏è La configuration contient encore des champs non support√©s.
‚û°Ô∏è V√©rifiez que vous utilisez bien la version corrig√©e du fichier.

### Pas de traces visibles

1. **V√©rifier que l'application envoie des traces** :
```bash
kubectl logs -n weeding-app deployment/backend-service | grep -i "otel\|trace"
```

2. **V√©rifier que le collector transmet √† Tempo** :
```bash
kubectl logs -n opentelemetry deployment/otel-collector | grep -i tempo
```

3. **V√©rifier que Tempo re√ßoit** :
```bash
kubectl logs -n opentelemetry deployment/tempo | grep -i "received\|ingesting"
```

### Traces incompl√®tes (spans manquants)

‚û°Ô∏è **Cause** : `trace_idle_period` trop court pour vos traces.

‚û°Ô∏è **Solution** : Augmenter dans la ConfigMap :
```yaml
ingester:
  trace_idle_period: 30s  # Au lieu de 20s
```

Puis red√©ployer :
```bash
kubectl rollout restart deployment/tempo -n opentelemetry
```

## üìù Limitations de cette Configuration

### Ce qui N'est PAS configur√© (car non support√© dans overrides)

- `max_trace_duration` : Dur√©e max d'une trace
- `max_bytes_per_trace` : Taille max d'une trace
- `max_spans_per_trace` : Nombre max de spans

**Impact** : Ces limites utilisent les valeurs par d√©faut de Tempo :
- max_trace_duration : ~30 minutes (largement suffisant)
- max_bytes_per_trace : ~5 MB (suffisant pour la plupart des traces Java)
- max_spans_per_trace : ~100,000 (tr√®s large)

### Si Vous Avez Besoin de Plus

Option 1 : **Passer √† Tempo 2.4+** qui supporte ces champs dans overrides

Option 2 : **Utiliser un fichier overrides s√©par√©** (configuration avanc√©e)

## ‚úÖ R√©sum√©

Votre configuration Tempo est maintenant :
- ‚úÖ **Syntaxiquement correcte** : Plus d'erreurs YAML
- ‚úÖ **Optimis√©e pour Java** : `trace_idle_period`, `max_block_bytes`
- ‚úÖ **Service graphs activ√©s** : Visualisation des relations
- ‚úÖ **Span metrics activ√©s** : M√©triques de latence et d'erreurs
- ‚úÖ **Compatible version 2.3.1** : Fonctionne avec l'image mise √† jour

Vous √™tes pr√™t √† recevoir et visualiser vos traces Java ! üéâ

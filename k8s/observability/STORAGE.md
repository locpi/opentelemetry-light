# Configuration Stockage Persistant - R√©tention 15 Jours

## üì¶ Vue d'Ensemble

La stack d'observabilit√© utilise maintenant des volumes persistants (PVC) pour stocker les donn√©es avec une r√©tention de 15 jours.

### R√©partition des Volumes (Total : 50 GB)

| Composant   | Stockage | R√©tention | Type de Donn√©es |
|-------------|----------|-----------|-----------------|
| Prometheus  | 15 GB    | 15 jours  | M√©triques       |
| Loki        | 15 GB    | 15 jours  | Logs            |
| Tempo       | 15 GB    | 15 jours  | Traces          |
| Grafana     | 5 GB     | Permanent | Dashboards/Config |
| **TOTAL**   | **50 GB**| -         | -               |

## üîß Configuration par Composant

### Prometheus (M√©triques - 15 GB)

**R√©tention configur√©e** :
```yaml
args:
  - '--storage.tsdb.retention.time=15d'    # 15 jours
  - '--storage.tsdb.retention.size=14GB'   # Max 14GB (marge de s√©curit√©)
```

**Volume** :
```yaml
volumes:
  - name: storage
    persistentVolumeClaim:
      claimName: prometheus-storage  # 15 GB
```

**Ce qui est conserv√© 15 jours** :
- M√©triques syst√®me (CPU, RAM, disque)
- M√©triques JVM (heap, threads, GC)
- M√©triques HTTP (requests, latency, errors)
- M√©triques custom de vos applications
- M√©triques g√©n√©r√©es par Tempo (span metrics, service graphs)

### Loki (Logs - 15 GB)

**R√©tention configur√©e** :
```yaml
limits_config:
  retention_period: 360h  # 15 jours (360 heures)

table_manager:
  retention_deletes_enabled: true
  retention_period: 360h

compactor:
  retention_enabled: true
  retention_delete_delay: 2h
```

**Volume** :
```yaml
volumes:
  - name: storage
    persistentVolumeClaim:
      claimName: loki-storage  # 15 GB
```

**Ce qui est conserv√© 15 jours** :
- Logs applicatifs de tous vos services
- Logs syst√®me Kubernetes
- Logs avec correlation trace_id
- Logs d'erreurs et exceptions

### Tempo (Traces - 15 GB)

**R√©tention configur√©e** :
```yaml
compactor:
  compaction:
    block_retention: 360h  # 15 jours (360 heures)
```

**Volume** :
```yaml
volumes:
  - name: storage
    persistentVolumeClaim:
      claimName: tempo-storage  # 15 GB
```

**Ce qui est conserv√© 15 jours** :
- Traces compl√®tes avec tous les spans
- Traces des appels entre microservices
- Traces des requ√™tes BDD
- Traces des appels HTTP
- M√©tadonn√©es Kubernetes associ√©es

### Grafana (Configuration - 5 GB)

**Volume** :
```yaml
volumes:
  - name: grafana-storage
    persistentVolumeClaim:
      claimName: grafana-storage  # 5 GB
```

**Ce qui est conserv√© de mani√®re permanente** :
- Dashboards cr√©√©s ou import√©s
- Configuration des datasources
- Utilisateurs et permissions
- Alertes configur√©es
- Annotations

## üöÄ D√©ploiement

### 1. Cr√©er les PersistentVolumeClaims

```bash
# Cr√©er les PVC (doit √™tre fait en premier)
kubectl apply -f k8s/observability/00-storage.yaml

# V√©rifier les PVC
kubectl get pvc -n opentelemetry
```

Vous devriez voir :
```
NAME                  STATUS   VOLUME                                     CAPACITY   ACCESS MODES
prometheus-storage    Bound    pvc-xxx-xxx-xxx                           15Gi       RWO
loki-storage         Bound    pvc-yyy-yyy-yyy                           15Gi       RWO
tempo-storage        Bound    pvc-zzz-zzz-zzz                           15Gi       RWO
grafana-storage      Bound    pvc-www-www-www                           5Gi        RWO
```

### 2. D√©ployer la Stack avec Stockage Persistant

```bash
# D√©ployer tous les composants
kubectl apply -f k8s/observability/

# V√©rifier que les pods utilisent les volumes
kubectl get pods -n opentelemetry
```

### 3. V√©rifier les Montages de Volumes

```bash
# Pour Prometheus
kubectl describe pod -n opentelemetry -l app=prometheus | grep -A 5 "Volumes:"

# Pour Loki
kubectl describe pod -n opentelemetry -l app=loki | grep -A 5 "Volumes:"

# Pour Tempo
kubectl describe pod -n opentelemetry -l app=tempo | grep -A 5 "Volumes:"

# Pour Grafana
kubectl describe pod -n opentelemetry -l app=grafana | grep -A 5 "Volumes:"
```

## üìä Estimation de l'Utilisation du Stockage

### Prometheus (15 GB)

Pour un cluster avec **5 services** instrument√©s :

| M√©trique | Estimation |
|----------|------------|
| Samples/sec | ~10,000 |
| Taille par sample | ~2 bytes |
| Par jour | ~1.7 GB |
| **15 jours** | **~25 GB** ‚ö†Ô∏è |

**Recommandation** : Avec 15 GB, vous pouvez conserver environ **8-9 jours** de m√©triques.

**Solutions** :
1. Augmenter le PVC √† 30 GB
2. R√©duire `scrape_interval` (actuellement 15s ‚Üí 30s)
3. Activer le downsampling

### Loki (15 GB)

Pour un cluster avec **5 services** g√©n√©rant des logs :

| M√©trique | Estimation |
|----------|------------|
| Logs/sec | ~100 |
| Taille moyenne | ~500 bytes |
| Par jour | ~4 GB (compress√©: ~800 MB) |
| **15 jours** | **~12 GB** ‚úÖ |

**Capacit√© suffisante** pour 15 jours avec compression.

### Tempo (15 GB)

Pour un cluster avec **5 services** avec traces :

| M√©trique | Estimation |
|----------|------------|
| Traces/sec | ~10 |
| Spans par trace | ~20 |
| Taille par span | ~2 KB |
| Par jour | ~3.5 GB (compress√©: ~700 MB) |
| **15 jours** | **~10 GB** ‚úÖ |

**Capacit√© suffisante** pour 15 jours.

### Grafana (5 GB)

| Contenu | Estimation |
|---------|------------|
| Dashboards | ~50 MB |
| Configuration | ~10 MB |
| Utilisateurs | ~1 MB |
| **Total** | **~100 MB** ‚úÖ |

**Capacit√© largement suffisante**.

## üìà Ajustements Recommand√©s

### Si Prometheus Manque d'Espace

#### Option 1 : Augmenter le Volume

```yaml
# Dans 00-storage.yaml
resources:
  requests:
    storage: 30Gi  # Au lieu de 15Gi
```

Puis :
```bash
kubectl delete pvc prometheus-storage -n opentelemetry
kubectl apply -f k8s/observability/00-storage.yaml
kubectl rollout restart deployment/prometheus -n opentelemetry
```

#### Option 2 : R√©duire l'Intervalle de Scraping

```yaml
# Dans 01-prometheus.yaml
global:
  scrape_interval: 30s  # Au lieu de 15s
```

**Impact** : R√©duit de 50% la consommation de stockage mais moins de granularit√©.

#### Option 3 : R√©duire la R√©tention

```yaml
args:
  - '--storage.tsdb.retention.time=10d'  # Au lieu de 15d
```

### Si Loki Manque d'Espace

```yaml
# Dans 02-loki.yaml
limits_config:
  retention_period: 240h  # 10 jours au lieu de 15
```

### Si Tempo Manque d'Espace

```yaml
# Dans 03-tempo.yaml
compactor:
  compaction:
    block_retention: 240h  # 10 jours au lieu de 15
```

## üîç Monitoring de l'Espace Disque

### V√©rifier l'Utilisation des PVC

```bash
# Script pour v√©rifier l'utilisation
for pod in $(kubectl get pods -n opentelemetry -o name); do
  echo "=== $pod ==="
  kubectl exec -n opentelemetry $pod -- df -h 2>/dev/null | grep -E '/prometheus|/loki|/var/tempo|/var/lib/grafana'
done
```

### Cr√©er des Alertes Prometheus

Ajoutez dans votre configuration Prometheus :

```yaml
# Alerte si le disque est √† 80%
- alert: HighDiskUsage
  expr: (1 - node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 > 80
  for: 5m
  labels:
    severity: warning
  annotations:
    summary: "Disk usage is above 80%"
```

### Dashboard Grafana pour le Stockage

Cr√©er un dashboard avec les requ√™tes :

```promql
# Utilisation Prometheus TSDB
prometheus_tsdb_storage_blocks_bytes / 1024 / 1024 / 1024

# Nombre de s√©ries Prometheus
prometheus_tsdb_head_series

# Taille des blocs Tempo
tempo_ingester_blocks_total
```

## üóëÔ∏è Nettoyage Manuel (si n√©cessaire)

### Forcer le Compactage Prometheus

```bash
kubectl exec -n opentelemetry deployment/prometheus -- \
  curl -X POST http://localhost:9090/api/v1/admin/tsdb/clean_tombstones
```

### Forcer le Compactage Loki

```bash
# Red√©marrer le compactor
kubectl rollout restart deployment/loki -n opentelemetry
```

### Forcer le Compactage Tempo

```bash
# Red√©marrer Tempo
kubectl rollout restart deployment/tempo -n opentelemetry
```

## üîÑ Migration depuis emptyDir

Si vous avez d√©j√† des donn√©es dans des volumes `emptyDir` :

### Sauvegarde des Donn√©es Grafana (Dashboards)

```bash
# Exporter les dashboards existants
kubectl port-forward -n opentelemetry svc/grafana 3000:3000 &
curl -H "Content-Type: application/json" \
  http://admin:admin@localhost:3000/api/search?type=dash-db \
  > dashboards-backup.json
```

### Restauration apr√®s Migration

Les donn√©es Prometheus, Loki et Tempo ne sont pas migrables facilement.
**Recommandation** : Accepter la perte des donn√©es historiques et red√©marrer avec les volumes persistants.

## üìù Backup et Restauration

### Backup des PVC (Recommand√©)

```bash
# Installer Velero pour les backups
kubectl apply -f https://github.com/vmware-tanzu/velero/releases/download/v1.12.0/velero-v1.12.0-linux-amd64.tar.gz

# Backup de tous les PVC
velero backup create observability-backup \
  --include-namespaces opentelemetry \
  --include-resources pvc,pv
```

### Snapshots R√©guliers (si support√© par votre cloud provider)

Pour AWS EBS, GCP Persistent Disk, ou Azure Disk :

```yaml
# VolumeSnapshot (exemple)
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: prometheus-snapshot
  namespace: opentelemetry
spec:
  volumeSnapshotClassName: csi-snapclass
  source:
    persistentVolumeClaimName: prometheus-storage
```

## ‚ö†Ô∏è Limitations et Consid√©rations

### Performances I/O

Les PVC sur Minikube utilisent le disque local :
- **Performances** : D√©pendent du disque h√¥te
- **IOPS** : Limit√©s par le storage backend

### Haute Disponibilit√©

Configuration actuelle :
- ‚ùå Pas de r√©plication
- ‚ùå Pas de backup automatique
- ‚ùå Single point of failure

Pour la production :
- ‚úÖ Utiliser des StatefulSets avec r√©plication
- ‚úÖ Configurer des backups automatiques
- ‚úÖ Utiliser un storage class avec r√©plication (EBS, GCE PD)

### Croissance du Stockage

Surveillez r√©guli√®rement :
```bash
# V√©rifier la croissance
kubectl exec -n opentelemetry deployment/prometheus -- \
  du -sh /prometheus

kubectl exec -n opentelemetry deployment/loki -- \
  du -sh /loki

kubectl exec -n opentelemetry deployment/tempo -- \
  du -sh /var/tempo
```

## üéØ R√©sum√©

‚úÖ **Configur√©** :
- 50 GB de stockage persistant r√©partis intelligemment
- R√©tention de 15 jours pour m√©triques, logs et traces
- Conservation permanente des dashboards Grafana
- Donn√©es persistantes m√™me en cas de red√©marrage des pods

‚ö†Ô∏è **√Ä Surveiller** :
- Utilisation du stockage Prometheus (peut atteindre la limite avant 15 jours)
- Croissance des donn√©es avec l'ajout de nouveaux services
- Performances I/O si beaucoup d'√©critures

üìä **Prochaines √âtapes** :
1. D√©ployer avec `kubectl apply -f k8s/observability/`
2. Monitorer l'utilisation du stockage
3. Ajuster les tailles de PVC si n√©cessaire
4. Configurer des backups r√©guliers
